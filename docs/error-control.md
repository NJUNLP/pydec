# Error Control
Although theoretically the results recovered from the composition are exactly equal to the ground truth, in practice, floating-point calculations introduce errors. Particularly in deep networks, these errors may be amplified to an unacceptable degree. We propose some recommendations for reducing errors.
$$
h \approx \frac{\mathscr{D}h}{\mathscr{D}\{x_1\}} + \cdots + \frac{\mathscr{D}h}{\mathscr{D}\{x_m\}} + \frac{\mathscr{D}h}{\mathscr{D}\{b^1\cdots\mathscr{D}b^L\}}.
$$

?> Call {{#auto_link}}pydec.Composition.c_sum short:1{{/auto_link}} to sum up each component and obtain the recovery of a composition.

## Error statistics
Here, we present the relative error between the output (Logits) and output decomposition of the [RoBERTa model](https://arxiv.org/abs/1907.11692) on the SST-2 and IMDB dataset.

*\* the relative error (%) with standard deviation*

| Precision | SST-2                | IMDB                |
| --------- | -------------------- | ------------------- |
| float16   | 0.526 (6.390)        | 0.243 (2.718)       |
| float32   | 1.723e-4 (2.821e-3)  | 1.411e04 (3.675e-3) |
| float64   | 2.664e-10 (1.200e-8) | 1.145e-9 (1.221e-8) |

In most cases, the error generated by float16 (<1%) is acceptable. However, the range of error fluctuation is large, and in rare cases it may change the predicted class of the model.

## Error Reduction

Our most recommended method for reducing errors is to use single or double precision calculation. Usually, you only need to add `model=model.float()` or `model=model.double()` after loading the model. If you don't care about error, you can use half precision to achieve faster inference speed and reduce memory usage.

Another way to reduce errors is to use the ground truth as a reference input to each operator in the forward propagation process, and the output of the operator will be accurate. However, this requires providing the ground truth of intermediate variables, so you need to modify a lot of model code to trace the forward propagation of tensors and compositions simultaneously.

Example:
```python
class TinyModel(torch.nn.Module):
    def __init__(self):
        super(TinyModel, self).__init__()
        self.linear1 = torch.nn.Linear(4, 10)
        self.linear2 = torch.nn.Linear(10, 2)

    def forward(self, x: torch.Tensor, c_x: pydec.Composition):
        c_x = self.linear1(c_x)
        x = self.linear1(x)
        # input the ground truth as reference
        c_x = pydec.nn.functional.relu(c_x, ref=x)
        x = torch.nn.functional.relu(x)
        assert (x - c_x.c_sum()).abs().mean() < 1e-5
        c_x = self.linear2(c_x)
        x = self.linear2(x)
        return x, c_x
```